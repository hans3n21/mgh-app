# Voice-to-Text Feature - Setup & Nutzung

## √úberblick

Mitarbeiter k√∂nnen in der Kommunikation-Ansicht eines Auftrags Sprachnachrichten aufnehmen, die automatisch in Text umgewandelt werden. Optional kann der Text dann via N8N professionalisiert werden.

## Workflow

```
1. Mitarbeiter klickt üé§-Button
2. Spricht Nachricht ein
3. Audio ‚Üí Whisper API (auf QNAP)
4. Text wird in Textarea eingef√ºgt
5. Optional: "Text verbessern" via N8N
6. Nachricht senden
```

## Installation

### Schritt 1: Whisper auf QNAP einrichten

```bash
# Via SSH auf QNAP
cd /share/Container/mgh-app

# docker-compose.yml wurde bereits erweitert
# Whisper-Container starten
docker-compose up -d whisper

# Logs pr√ºfen (Modell-Download dauert 2-5 Min)
docker-compose logs -f whisper
```

**Erwartete Ausgabe:**
```
whisper-stt | Downloading model 'base'...
whisper-stt | Model loaded successfully
whisper-stt | Server running on http://0.0.0.0:9000
```

### Schritt 2: Umgebungsvariable setzen

In `.env.local` auf dem QNAP:

```bash
# Whisper API (Container-internes Netzwerk)
WHISPER_API_URL="http://whisper:9000"

# Optional: N8N f√ºr Text-Versch√∂nerung
N8N_WEBHOOK_URL="https://your-n8n.com/webhook/compose-message"
```

### Schritt 3: App neu deployen

```bash
# Auf QNAP
cd /share/Container/mgh-app
docker-compose down
docker-compose up -d
```

## Verwendung

### In der App

1. Auftrag √∂ffnen ‚Üí Tab "Kommunikation"
2. Neue Nachricht schreiben
3. üé§-Button klicken
4. Sprechen (z.B. "Kunde hat angerufen, Material verz√∂gert, neue Lieferzeit Kalenderwoche f√ºnfzehn")
5. Stopp-Button klicken
6. Text wird automatisch eingef√ºgt
7. Optional bearbeiten oder direkt senden

### Browser-Berechtigungen

Beim ersten Mal fragt der Browser nach Mikrofon-Zugriff.

**Wichtig:** HTTPS ist erforderlich f√ºr Mikrofon-Zugriff (au√üer localhost).

## Technische Details

### Unterst√ºtzte Audio-Formate

- WebM (Browser-Standard)
- Whisper konvertiert automatisch

### Sprachen

Standardm√§√üig Deutsch (`de`), kann in der Komponente ge√§ndert werden:

```typescript
<VoiceInputButton
  language="en" // oder "de", "fr", "es", etc.
  onTranscript={(text) => ...}
/>
```

### Performance

**Whisper Base Model auf TS-432X:**
- 10 Sekunden Audio = ~8-12 Sekunden Verarbeitung
- 30 Sekunden Audio = ~25-40 Sekunden Verarbeitung

**Schneller mit `tiny` Model:**

In `docker-compose.yml`:
```yaml
whisper:
  environment:
    - ASR_MODEL=tiny  # statt base
```

Dann `docker-compose restart whisper`

## Troubleshooting

### Button macht nichts

**Browser-Konsole pr√ºfen:**
```
F12 ‚Üí Console
```

**H√§ufige Fehler:**
- `NotAllowedError`: Mikrofon-Berechtigung verweigert
- `NotFoundError`: Kein Mikrofon gefunden
- `NotSecureContext`: HTTP statt HTTPS (au√üer localhost)

**L√∂sung:**
- Berechtigungen pr√ºfen (Browser-Einstellungen ‚Üí Mikrofon)
- HTTPS nutzen oder via localhost testen

### Whisper API nicht erreichbar

```bash
# Auf QNAP: Container-Status pr√ºfen
docker ps | grep whisper

# Sollte "Up" sein, z.B.:
# whisper-stt   Up 10 minutes   0.0.0.0:9000->9000/tcp

# Logs pr√ºfen
docker logs whisper-stt

# Von QNAP aus testen
curl http://localhost:9000/
# Sollte antworten: "Whisper ASR Webservice is up and running!"
```

### Schlechte Transkriptions-Qualit√§t

**Optionen:**

1. **Gr√∂√üeres Modell** (besser, aber langsamer):
   ```yaml
   # docker-compose.yml
   whisper:
     environment:
       - ASR_MODEL=small  # statt base
   ```

2. **Sprache explizit setzen** (hilft bei Dialekten):
   ```typescript
   <VoiceInputButton language="de" />
   ```

3. **Deutlich sprechen:**
   - Nicht zu schnell
   - Klare Aussprache
   - Ruhige Umgebung

### Audio-Datei zu gro√ü

Falls Aufnahmen l√§nger als 1 Minute:

In `app/api/voice-to-text/route.ts` kann man Limits setzen.

**Empfehlung:** Mehrere kurze Aufnahmen statt einer langen.

## N8N Text-Versch√∂nerung (Optional)

### Setup

1. N8N Workflow erstellen (siehe `N8N_WORKFLOW.md`)
2. Webhook-URL in `.env.local` eintragen
3. App-Container neu starten

### Automatische Nutzung

Nach Voice-Transkription k√∂nnte man automatisch N8N aufrufen:

```typescript
// In VoiceInputButton.tsx nach erfolgreicher Transkription
const improveText = async (rawText: string) => {
  const res = await fetch('/api/compose-message', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text: rawText }),
  });
  const data = await res.json();
  return data.text || rawText;
};
```

### Manueller "Verbessern"-Button

Kann in `MessageSystem.tsx` erg√§nzt werden (siehe N8N_WORKFLOW.md).

## Sicherheit

### Datenschutz

**Whisper l√§uft lokal auf dem NAS:**
- Kein Cloud-Service
- Keine Daten verlassen das NAS
- DSGVO-konform

**N8N (optional):**
- Wenn N8N lokal l√§uft: Auch DSGVO-konform
- Cloud-LLM (OpenAI/Claude): Daten werden extern verarbeitet
  ‚Üí Ggf. Kunden informieren oder Self-Hosted LLM nutzen (Ollama)

### HTTPS

F√ºr Produktion **unbedingt HTTPS** nutzen:

**Option A: Reverse Proxy (nginx/Caddy)**
```nginx
server {
    listen 443 ssl;
    server_name mgh.your-domain.com;

    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;

    location / {
        proxy_pass http://localhost:4000;
    }
}
```

**Option B: Cloudflare Tunnel**
- Kostenlos
- Automatisches SSL
- Kein Port-Forwarding n√∂tig

## Erweiterungen

### 1. Sprachauswahl

Dropdown vor Voice-Button:

```typescript
const [language, setLanguage] = useState<'de' | 'en'>('de');

<select value={language} onChange={(e) => setLanguage(e.target.value)}>
  <option value="de">üá©üá™ Deutsch</option>
  <option value="en">üá¨üáß English</option>
</select>

<VoiceInputButton language={language} ... />
```

### 2. Audio-Vorschau

Vor dem Senden Audio abspielen:

```typescript
// In VoiceInputButton nach Aufnahme
const audioUrl = URL.createObjectURL(audioBlob);
<audio src={audioUrl} controls />
```

### 3. Offline-Fallback

Falls Whisper nicht verf√ºgbar:

```typescript
// In VoiceInputButton
if (!response.ok && response.status === 503) {
  // Whisper nicht verf√ºgbar
  // Browser Web Speech API als Fallback?
  const recognition = new (window as any).webkitSpeechRecognition();
  // ...
}
```

## Performance-Tuning

### Whisper-Modell-Vergleich

| Modell | Gr√∂√üe | RAM | Geschwindigkeit | Qualit√§t (DE) |
|--------|-------|-----|-----------------|---------------|
| tiny   | 75 MB | 500 MB | 2-3x schneller | ‚≠ê‚≠ê |
| base   | 150 MB | 1 GB | Standard | ‚≠ê‚≠ê‚≠ê |
| small  | 500 MB | 2 GB | 0.5x | ‚≠ê‚≠ê‚≠ê‚≠ê |
| medium | 1.5 GB | 4 GB | 0.3x | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

**Empfehlung f√ºr TS-432X:** `base` (guter Kompromiss)

### Container-Ressourcen

Falls RAM knapp:

```yaml
whisper:
  deploy:
    resources:
      limits:
        memory: 2G  # Max 2 GB RAM
      reservations:
        memory: 1G  # Min 1 GB RAM
```

## Roadmap / Ideen

- [ ] Audio-Clips speichern (f√ºr Training/Qualit√§t)
- [ ] Mehrere Sprachen auto-detect
- [ ] Echtzeit-Transkription (w√§hrend Aufnahme)
- [ ] Voice-Commands ("Senden", "Abbrechen")
- [ ] Speaker Diarization (wer hat was gesagt)
- [ ] Automatische Punkt-Setzung

## Support

Bei Problemen:

1. Logs pr√ºfen: `docker-compose logs whisper`
2. API testen: `curl http://localhost:9000/`
3. Browser-Konsole pr√ºfen (F12)
4. Issue erstellen mit Logs

## Siehe auch

- [WHISPER_SETUP.md](../WHISPER_SETUP.md) - Detailliertes Whisper-Setup
- [N8N_WORKFLOW.md](./N8N_WORKFLOW.md) - N8N Text-Versch√∂nerung
- [Whisper.cpp GitHub](https://github.com/ggerganov/whisper.cpp)
